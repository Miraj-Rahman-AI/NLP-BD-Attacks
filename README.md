# NLP-BD-Attacks

**NLP-BD-Attacks** is a research-oriented toolkit designed for studying **backdoor attacks** in **Natural Language Processing (NLP)** models, particularly in **federated learning** and decentralized environments. This repository provides implementations of multiple attack strategies, transformer and LSTM-based model architectures, training and evaluation scripts, and dataset configurations focused on security vulnerabilities in NLP systems.

## ğŸ¯ Project Objectives

- ğŸ’£ Simulate **backdoor attacks** in NLP models (e.g., text classification).
- ğŸ” Support **federated learning environments** with client-specific poisoning.
- ğŸ“Š Provide tools to evaluate model behavior under **poisoned vs. clean** conditions.
- ğŸ§  Enable research and experimentation on **secure and robust NLP**.

## ğŸ“ Repository Structure
```NLP-BD-Attacks/ 
â”‚ â”œâ”€â”€ models/ # Model definitions 
â”‚ â”œâ”€â”€ TransformerModel.py # Transformer for NLP 
â”‚ â”œâ”€â”€ word_model.py # Word-based classifier 
â”‚ â”œâ”€â”€ simple.py # Baseline models 
â”‚ â””â”€â”€ ... # ResNet/CIFAR-related residual models 
â”‚ â”œâ”€â”€ utils/ # YAML-based poisoned word dictionaries 
â”‚ â”œâ”€â”€ words_IMDB.yaml # Backdoor trigger words for IMDB dataset 
â”‚ â”œâ”€â”€ words_reddit_*.yaml # Reddit triggers for GPT2, LSTM, Transformer 
â”‚ â””â”€â”€ text_load.py # Load text datasets and vocabulary 
â”‚ â”œâ”€â”€ IMDB.py # IMDB dataset wrapper
â”œâ”€â”€ main_training.py # Main model training script 
â”œâ”€â”€ run_NLP_tasks.sh # Shell script for executing NLP tasks 
â”œâ”€â”€ helper.py # Model and trigger handling 
â”œâ”€â”€ test_funcs.py # Evaluation functions 
â”œâ”€â”€ train_funcs.py # Training logic 
â”œâ”€â”€ text_helper.py # Text preprocessing helpers 
â”œâ”€â”€ write_script_nlp.py # Logging and reproducibility scripts 
â””â”€â”€ README.md # Project documentation
```


## âš™ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/Miraj-Rahman-AI/NLP-BD-Attacks.git
cd NLP-BD-Attacks
```
## ğŸ“ˆ Example: Results Snapshot

| **Setup**             | **Trigger**     | **Target** | **Clean Acc.** | **Backdoor Acc.** |
|-----------------------|-----------------|------------|----------------|-------------------|
| IMDB - 5 clients      | quantumflux     | 1          | 91.3%          | 97.6%             |
| Reddit - 10 clients   | gnosiskey       | 3          | 87.8%          | 93.2%             |
| Sent140 - 5 clients   | lightform       | 0          | 89.5%          | 94.9%             |

## ğŸ” Supported Features

### ğŸ§  Model Architectures
- âœ… **Transformers** (e.g., BERT-style and custom variants)
- âœ… **LSTM / GRU / SimpleRNN**
- âœ… **Word-level CNN/MLP classifiers**
- âœ… **GPT2** (for Reddit dataset compatibility)


### ğŸ’¥ Attack Capabilities
- Trigger word insertion (via YAML)
- Client-specific label poisoning
- Task coverage: Sentiment classification, next-word prediction

### ğŸ§° Evaluation Tools
- Clean vs. Poisoned Accuracy
- Client-level behavior reports
- Trigger injection effectiveness
- Model confusion tracking

## ğŸ§ª Ideal Use Cases

- ğŸ“ Graduate-level research on secure NLP
- ğŸ” Investigating vulnerabilities in FL-based LLMs
- ğŸ›¡ï¸ Designing and benchmarking backdoor defenses
- ğŸ§¬ Prototyping novel attack scenarios in NLP security

## ğŸ“š References
- **Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., & Shmatikov, V.**  
  *How to Backdoor Federated Learning*  
  In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS), 2020.  
  [Paper Link](https://proceedings.neurips.cc/paper/2020/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf)

- **Kurita, K., Michel, P., & Neubig, G.**  
  *Weight Poisoning Attacks on Pre-trained Models*  
  In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), 2020.  
  [Paper Link](https://aclanthology.org/2020.acl-main.460/)

- **Salem, A., Bourtoule, L., Zhang, P., Rubinstein, B. I. P., & Papernot, N.**  
  *Dynamic Backdoor Attacks Against NLP Models*  
  In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.  
  [Paper Link](https://aclanthology.org/2021.emnlp-main.98/)

- **Chen, J., Dai, X., Li, H., Xiao, X., & Chen, X.**  
  *BadNL: Backdoor Attacks Against NLP Models*  
  In Proceedings of the 28th International Conference on Computational Linguistics (COLING), 2020.  
  [Paper Link](https://aclanthology.org/2020.coling-main.512/)

